{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "#from ggplot import *\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "macro_cols = [\"balance_trade\", \"balance_trade_growth\", \"eurrub\", \"average_provision_of_build_contract\",\n",
    "\"micex_rgbi_tr\", \"micex_cbi_tr\", \"deposits_rate\", \"mortgage_value\", \"mortgage_rate\",\n",
    "\"income_per_cap\", \"rent_price_4+room_bus\", \"museum_visitis_per_100_cap\", \"apartment_build\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv', parse_dates=['timestamp'])\n",
    "test = pd.read_csv('../input/test.csv', parse_dates=['timestamp'])\n",
    "macro = pd.read_csv('../input/macro.csv', parse_dates=['timestamp'],usecols=['timestamp'] + macro_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['LogAmt']=np.log(train.price_doc+1.0)\n",
    "print(train['LogAmt'].describe())\n",
    "sns.distplot(train['LogAmt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Merge data into one dataset to prepare compare between train and test\n",
    "train_1 = train.copy()\n",
    "train_1['Source']='Train'\n",
    "test_1 = test.copy()\n",
    "test_1['Source']='Test'\n",
    "alldata = pd.concat([train_1, test_1],ignore_index=True)\n",
    "\n",
    "macro.columns = ['mac__'+c if c!='timestamp' else 'timestamp' for c in macro.columns ]\n",
    "alldata=alldata.merge(macro,on='timestamp',how='left')\n",
    "print(alldata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def var_desc(dt,alldata):\n",
    "    print('--------------------------------------------')\n",
    "    for c in alldata.columns:\n",
    "        if alldata[c].dtype==dt:\n",
    "            t1 = alldata[alldata.Source=='Train'][c]\n",
    "            t2 = alldata[alldata.Source=='Test'][c]\n",
    "            if dt==\"object\":\n",
    "                f1 = t1[pd.isnull(t1)==False].value_counts()\n",
    "                f2 = t2[pd.isnull(t2)==False].value_counts()\n",
    "            else:\n",
    "                f1 = t1[pd.isnull(t1)==False].describe()\n",
    "                f2 = t2[pd.isnull(t2)==False].describe()\n",
    "            m1 = t1.isnull().value_counts()\n",
    "            m2 = t2.isnull().value_counts()\n",
    "            f = pd.concat([f1, f2], axis=1)\n",
    "            m = pd.concat([m1, m2], axis=1)\n",
    "            f.columns=['Train','Test']\n",
    "            m.columns=['Train','Test']\n",
    "            print(dt+' - '+c)\n",
    "            print('UniqValue - ',len(t1.value_counts()),len(t2.value_counts()))\n",
    "            print(f.sort_values(by='Train',ascending=False))\n",
    "            print()\n",
    "\n",
    "            m_print=m[m.index==True]\n",
    "            if len(m_print)>0:\n",
    "                print('missing - '+c)\n",
    "                print(m_print)\n",
    "            else:\n",
    "                print('NO Missing values - '+c)\n",
    "            if dt!=\"object\":\n",
    "                if len(t1.value_counts())<=10:\n",
    "                    c1 = t1.value_counts()\n",
    "                    c2 = t2.value_counts()\n",
    "                    c = pd.concat([c1, c2], axis=1)\n",
    "                    f.columns=['Train','Test']\n",
    "                    print(c)\n",
    "            print('--------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Uncomment to run variable description\n",
    "var_desc('object',alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## convert obj to num\n",
    "for c in alldata.columns:\n",
    "    if alldata[c].dtype=='object' and c not in ['sub_area','timestamp','Source']:\n",
    "        if len(alldata[c].value_counts())==2:\n",
    "            alldata['num_'+c]=[0 if x in ['no','OwnerOccupier'] else 1 for x in alldata[c]]\n",
    "        if len(alldata[c].value_counts())==5:\n",
    "            alldata['num_'+c]=0\n",
    "            alldata['num_'+c].loc[alldata[c]=='poor']=0\n",
    "            alldata['num_'+c].loc[alldata[c]=='satisfactory']=1\n",
    "            alldata['num_'+c].loc[alldata[c]=='good']=2\n",
    "            alldata['num_'+c].loc[alldata[c]=='excellent']=3\n",
    "            alldata['num_'+c].loc[alldata[c]=='no data']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excl_col=['id','timestamp','sub_area'] + [c for c in alldata.columns if alldata[c].dtype=='object']\n",
    "resv_col=['price_doc','LogAmt','Source','cafe_sum_500_max_price_avg','cafe_sum_500_min_price_avg','cafe_avg_price_500','hospital_beds_raion']\n",
    "def sel_grp(keys):\n",
    "    lst_all = list()\n",
    "    for k in keys:\n",
    "        lst = [c for c in alldata.columns if c.find(k)!=-1 and c not in excl_col and c not in resv_col]\n",
    "        lst = list(set(lst))\n",
    "        lst_all += lst\n",
    "    return(lst_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_grp = dict({})\n",
    "col_grp['people']=sel_grp(['_all','male'])\n",
    "col_grp['id'] = sel_grp(['ID_'])\n",
    "col_grp['church']=sel_grp(['church'])\n",
    "col_grp['build']=sel_grp(['build_count_'])\n",
    "col_grp['cafe']=sel_grp(['cafe_count'])\n",
    "col_grp['cafeprice']=sel_grp(['cafe_sum','cafe_avg'])\n",
    "col_grp['km']=sel_grp(['_km','metro_min','_avto_min','_walk_min','_min_walk'])\n",
    "col_grp['mosque']=sel_grp(['mosque_count'])\n",
    "col_grp['market']=sel_grp(['market_count'])\n",
    "col_grp['office']=sel_grp(['office_count'])\n",
    "col_grp['leisure']=sel_grp(['leisure_count'])\n",
    "col_grp['sport']=sel_grp(['sport_count'])\n",
    "col_grp['green']=sel_grp(['green_part'])\n",
    "col_grp['prom']=sel_grp(['prom_part'])\n",
    "col_grp['trc']=sel_grp(['trc_count'])\n",
    "col_grp['sqm']=sel_grp(['_sqm_'])\n",
    "col_grp['raion']=sel_grp(['_raion'])\n",
    "col_grp['macro']=sel_grp(['mac__'])\n",
    "col_grp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_tmp = list()\n",
    "for d in col_grp:\n",
    "    col_tmp+=(col_grp[d])\n",
    "col_grp['other']=[c for c in alldata.columns if c not in col_tmp and c not in excl_col and c not in resv_col]\n",
    "col_grp['other'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "macro_missing_2 = pd.DataFrame([[c,sum(alldata[c].isnull())] for c in col_grp['macro']],columns=['Var','Missing'])\n",
    "macro_missing_3=macro_missing_2[macro_missing_2.Missing>5000]\n",
    "print(macro_missing_3)\n",
    "excl_col+=list(macro_missing_3.Var)\n",
    "print(excl_col)\n",
    "\n",
    "col_grp['macro']=sel_grp(['mac__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loopkeys=list(col_grp.keys())\n",
    "print(loopkeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partial_pca(var,data,col_grp):\n",
    "    from sklearn.decomposition import PCA\n",
    "    import bisect\n",
    "    pca = PCA()\n",
    "    df = data[col_grp[var]].dropna()\n",
    "    print([len(data[col_grp[var]]), len(df)])\n",
    "    df = (df-df.mean())/df.std(ddof=0)\n",
    "    pca.fit(df)\n",
    "    varexp = pca.explained_variance_ratio_.cumsum()\n",
    "    cutoff = bisect.bisect(varexp, 0.95)\n",
    "    #print(cutoff)\n",
    "    #print(pca.explained_variance_ratio_.cumsum())\n",
    "    newcol=pd.DataFrame(pca.fit_transform(X=df)[:,0:(cutoff+1)],columns=['PCA_'+var+'_'+str(i) for i in range(cutoff+1)],index=df.index)\n",
    "    #print(newcol)\n",
    "    col_grp['PCA_'+var]=list(newcol.columns)\n",
    "    return(newcol,col_grp,pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in loopkeys:\n",
    "    if c not in ['other','macro']:\n",
    "        print(c)\n",
    "        newcol,col_grp,pca = partial_pca(c,alldata,col_grp)\n",
    "        alldata=alldata.join(newcol)\n",
    "        print(alldata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wpca=list()\n",
    "wopca=list()\n",
    "for c in col_grp.keys():\n",
    "    if c.find('PCA_')!=-1:\n",
    "        wpca+=col_grp[c]\n",
    "    else:\n",
    "        wopca+=col_grp[c]\n",
    "        \n",
    "wpca+=col_grp['other']\n",
    "wpca+=resv_col\n",
    "wopca+=col_grp['other']\n",
    "wopca+=resv_col\n",
    "\n",
    "wpca=list(set(wpca))\n",
    "wopca=list(set(wopca))\n",
    "\n",
    "wpca.sort()\n",
    "wopca.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(set(wpca+wopca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Add a few more features suggested by other discussions\n",
    "##\n",
    "# Add month-year\n",
    "month_year = (alldata.timestamp.dt.month + alldata.timestamp.dt.year * 100)\n",
    "month_year_cnt_map = month_year.value_counts().to_dict()\n",
    "alldata['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
    "\n",
    "# Add week-year count\n",
    "week_year = (alldata.timestamp.dt.weekofyear + alldata.timestamp.dt.year * 100)\n",
    "week_year_cnt_map = week_year.value_counts().to_dict()\n",
    "alldata['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
    "\n",
    "# Add month and day-of-week\n",
    "alldata['month'] = alldata.timestamp.dt.month\n",
    "alldata['dow'] = alldata.timestamp.dt.dayofweek\n",
    "\n",
    "# Other feature engineering\n",
    "alldata['rel_floor'] = alldata['floor'] / alldata['max_floor'].astype(float)\n",
    "alldata['rel_kitch_sq'] = alldata['kitch_sq'] / alldata['full_sq'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wpca +=['month_year_cnt','week_year_cnt','dow','month','rel_floor','rel_kitch_sq']\n",
    "wopca+=['month_year_cnt','week_year_cnt','dow','month','rel_floor','rel_kitch_sq']\n",
    "allfeature=list(set(wpca+wopca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 12,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1,\n",
    "    'min_child_weight': 200\n",
    "}\n",
    "\n",
    "def cv_xgb(val_train_X,val_train_Y,val_val_X,val_val_Y):\n",
    "    dtrain = xgb.DMatrix(val_train_X, val_train_Y, feature_names=val_train_X.columns)\n",
    "    dval = xgb.DMatrix(val_val_X, val_val_Y, feature_names=val_val_X.columns)\n",
    "\n",
    "    # Uncomment to tune XGB `num_boost_rounds`\n",
    "    partial_model = xgb.train(xgb_params, dtrain, num_boost_round=1000, evals=[(dval, 'val')],\n",
    "                           early_stopping_rounds=50, verbose_eval=20)\n",
    "\n",
    "    num_boost_round = partial_model.best_iteration\n",
    "    return(num_boost_round,partial_model.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## let's try a 5-fold CV\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(5,shuffle =True)\n",
    "\n",
    "train_col = [c for c in alldata[wpca].columns if c not in ['price_doc','Source']]\n",
    "alldata_1 = alldata[alldata.Source=='Train'][train_col]\n",
    "\n",
    "for val_train, val_val in kf.split(alldata_1):\n",
    "    val_train_X = alldata_1.ix[val_train].drop('LogAmt',axis=1)\n",
    "    val_train_Y = alldata_1.ix[val_train].LogAmt\n",
    "    val_val_X = alldata_1.ix[val_val].drop('LogAmt',axis=1)\n",
    "    val_val_Y = alldata_1.ix[val_val].LogAmt\n",
    "    dtrain = xgb.DMatrix(val_train_X, val_train_Y, feature_names=val_train_X.columns)\n",
    "    dval = xgb.DMatrix(val_val_X, val_val_Y, feature_names=val_val_X.columns)\n",
    "\n",
    "    # Uncomment to tune XGB `num_boost_rounds`\n",
    "    partial_model = xgb.train(xgb_params, dtrain, num_boost_round=1000, evals=[(dval, 'val')],\n",
    "                           early_stopping_rounds=50, verbose_eval=20)\n",
    "\n",
    "    num_boost_round = partial_model.best_iteration\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Run it on the full model \n",
    "all_train_X = alldata_1.drop('LogAmt',axis=1)\n",
    "all_train_Y = alldata_1.LogAmt\n",
    "all_test_X = alldata[alldata.Source=='Test'][train_col].drop('LogAmt',axis=1)\n",
    "dtrain_all = xgb.DMatrix(all_train_X, all_train_Y, feature_names=all_train_X.columns)\n",
    "dtest      = xgb.DMatrix(all_test_X, feature_names=all_test_X.columns)\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain_all, num_boost_round=num_boost_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 16))\n",
    "xgb.plot_importance(model, height=0.5, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Make a predicition\n",
    "ylog_pred = model.predict(dtest)\n",
    "y_pred = np.exp(ylog_pred) - 1\n",
    "id_test = alldata[alldata.Source=='Test'].id\n",
    "df_sub = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\n",
    "df_sub.to_csv('sub_pca.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Top 20 correlated variables\n",
    "corrmat = alldata[wpca].corr()\n",
    "k = 20 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'price_doc')['price_doc'].index\n",
    "cm = alldata[cols].corr()\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=False, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
